{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7a646c4-5cb2-4d37-a454-b96e41d8bf72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/Deeplab\n",
      "<class 'boto3.resources.factory.s3.Bucket'>\n",
      "11769\n",
      "haiti/ACACAURI/2020.12.06.10.40.43_18.29333297442645_-73.5601465869695_0cee5eeb-2073-4ee2-8793-648519bba4d2_IMG_20201202_113120_4068726501778222474.jpg\n",
      "12515\n",
      "https://treetracker-production-images.s3.eu-central-1.amazonaws.com/2021.01.09.07.47.44_10.357156977999999_77.97936750799998_7354043b-c838-48ed-809c-ec008bad3f36_IMG_20210109_074718_61131148489065839.jpg\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "env_path = '/teamspace/studios/this_studio/Deeplab/credentials.env'\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "import boto3\n",
    "\n",
    "%cd /teamspace/studios/this_studio/Deeplab/\n",
    "from inference.infer import *\n",
    "from post_processing.control_random_field import *\n",
    "\n",
    "bucket = get_s3_bucket('treetracker-training-images')\n",
    "print(type(bucket))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#img_link = 'https://treetracker-production-images.s3.eu-central-1.amazonaws.com/2021.03.28.12.34.59_-4.531118333333334_38.248731666666664_6973c014-d0e7-4fa8-a80f-9495bfc8af3d_IMG_20210322_094829_8180242403936928723.jpg'\n",
    "inhouse_default = '/teamspace/studios/this_studio/Deeplab/saved_models/best_deeplabv3plus_mobilenet_custom_os16_0.7854892764326529.pth'\n",
    "downstreamed_large = '/teamspace/studios/this_studio/Deeplab/saved_models/india_sam_dino_250_samples_0.8495187035894669.pth'\n",
    "downstreamed_small = '/teamspace/studios/this_studio/Deeplab/saved_models/india_sam_dino_250_samples_0.8940880817269634.pth'\n",
    "downstreamed_tuned = '/teamspace/studios/this_studio/Deeplab/saved_models/base_sam_dino_india_tuned_freetown_eastafrica_haiti_0.8213798905479425.pth'\n",
    "crf_large = '/teamspace/studios/this_studio/Deeplab/saved_models/crf_sam_large_freetown_eastafrica_haiti_0.8385291022474306.pth'\n",
    "crf_proper = '/teamspace/studios/this_studio/Deeplab/saved_models/crf_sam_large_freetown_eastafrica_haiti_0.8696894799916581.pth'\n",
    "crf_small = '/teamspace/studios/this_studio/Deeplab/saved_models/base_sam_dino_india_tuned_freetown_eastafrica_haiti_0.8754172925254217.pth'\n",
    "\n",
    "#link = 'https://treetracker-production-images.s3.eu-central-1.amazonaws.com/2021.02.28.18.13.02_25.26227899999999_80.98948_997849e4-3ab5-4559-afed-8a795b9b16c4_IMG_20210228_083947_5970478655671293036.jpg'\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame  freetown_deeplab_confidence_processed.csv\n",
    "df = pd.read_csv('/teamspace/studios/this_studio/pilot_with_crf_large_11769.csv')\n",
    "\n",
    "file_list = df['File'].tolist()\n",
    "print(len(file_list))\n",
    "print(file_list[0])\n",
    "\n",
    "\n",
    "# Read the CSV file into a DataFrame  freetown_deeplab_confidence_processed.csv\n",
    "df = pd.read_csv('/teamspace/studios/this_studio/india_deeplab_confidence_processed.csv')\n",
    "\n",
    "url_list = df['image_url'].tolist()\n",
    "print(len(url_list))\n",
    "print(url_list[0])\n",
    "\n",
    "def show_mask(img_np, pred, input_box = None):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    # Create a blue mask based on the prediction\n",
    "    blue_mask = np.zeros_like(img_np)\n",
    "    blue_mask[:, :, 2] = pred * 255\n",
    "\n",
    "    # Plot the image and the mask overlay\n",
    "    plt.imshow(img_np)\n",
    "    plt.imshow(blue_mask, alpha=0.7)\n",
    "    if input_box is not None:\n",
    "        show_box(input_box, plt.gca())\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.gca().xaxis.set_major_locator(plt.NullLocator())\n",
    "    plt.gca().yaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "fdc315a8-d2cc-4c95-b4f4-8cfeb2e739ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overlayed_mask(checkpoint, to_download, use_url=False, my_bucket = None, save_binary_mask=False):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = load_deeplab_model(checkpoint, device).eval()\n",
    "    img_path = '/teamspace/studios/this_studio/test.jpg'\n",
    "    if use_url:\n",
    "        \n",
    "        urllib.request.urlretrieve(to_download, img_path)\n",
    "        #!wget -q $to_download -O $img_path\n",
    "    else:\n",
    "        #my_bucket.download_file(to_download, '/teamspace/studios/this_studio/test.jpg')\n",
    "        img_path = to_download  # Assuming `to_download` contains the local path in this case\n",
    "    flip(img_path,img_path)\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    img_tensor = img_transform(img).unsqueeze(0).to(device, dtype=torch.float32)\n",
    "    #return img_tensor\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor)\n",
    "        #return output\n",
    "        output = torch.squeeze(output, dim=1)\n",
    "        #return output\n",
    "        prob = torch.sigmoid(output)\n",
    "        #return prob\n",
    "        pred = (prob > 0.5).squeeze(0)\n",
    "        #return pred\n",
    "\n",
    "    denorm = utils.Denormalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    img_np = img_tensor[0].detach().cpu().numpy()\n",
    "    img_np = (denorm(img_np) * 255).transpose(1, 2, 0).astype(np.uint8)\n",
    "\n",
    "    prob_np = prob[0].cpu().numpy()\n",
    "    #return prob_np\n",
    "    count = np.sum(prob_np > 0.5)\n",
    "    confidence = np.sum(prob_np[prob_np > 0.5]) / count if count != 0 else 0\n",
    "    return pred\n",
    "    if count == 0:\n",
    "        print('Nothing detected')\n",
    "    else:\n",
    "        show(img)\n",
    "        print('confidence is ' + str(confidence))\n",
    "        show_mask(img_np, pred)\n",
    "        cleaned_mask = crf_with_prob(img_np, pred*255, prob_np)\n",
    "        show_mask(img_np, cleaned_mask)\n",
    "    if save_binary_mask:\n",
    "        pred_rgb = np.array(decode_target(pred)).astype(np.uint8)\n",
    "        pred_image = Image.fromarray(pred_rgb).convert('RGB')\n",
    "        pred_image.save(os.path.join(os.getcwd(), 'test_binary.jpg'))\n",
    "        print('Binary Mask saved to ' + os.path.join(os.getcwd(), 'test_binary.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e79522b9-887b-403f-9c5c-69c98576992f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '/teamspace/studios/this_studio/Deeplab/crf_sam_annotations/samples/eastafrica_acactort_2020.05.08.14.52.18_820f49c8-795b-453e-8eb1-7d473669ab84_img_20200507_152251_66198747.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "278d8994-3c4a-49c6-bc2d-b3bc8119d1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Resume model from /teamspace/studios/this_studio/Deeplab/saved_models/best_deeplabv3plus_mobilenet_custom_os16_0.7854892764326529.pth\n",
      "No ExifTags found: 'str' object has no attribute '_getexif'\n"
     ]
    }
   ],
   "source": [
    "output = get_overlayed_mask(inhouse_default,img_path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4efaf225-a0d6-41ff-b571-33bfdf1dcb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b3f323fc-999e-4fcc-88ed-a36ba20d87b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512)\n"
     ]
    }
   ],
   "source": [
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a594a9d-978e-4a98-a65a-773030f5397d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL import Image\n",
    "from torchvision.io import read_image, write_jpeg\n",
    "from torchvision.ops import masks_to_boxes\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "mask_transform = transforms.Compose([\n",
    "            #RandomCropAndPadMask(512),\n",
    "            transforms.Resize((512, 512), interpolation=transforms.InterpolationMode.NEAREST),\n",
    "            #transforms.RandomResizedCrop(size=(256, 256), interpolation=transforms.InterpolationMode.NEAREST),\n",
    "            #transforms.RandomHorizontalFlip(),\n",
    "            #transforms.RandomRotation(degrees=(0, 360)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "def get_target(mask_path):\n",
    "    mask = Image.open(mask_path)\n",
    "\n",
    "    mask_array = np.array(mask)\n",
    "    mask_array = (mask_array > 128).astype(np.uint8) # Binarize to 0s and 1s\n",
    "\n",
    "    mask_array = mask_array * 255\n",
    "    mask = Image.fromarray(mask_array.astype(np.uint8))\n",
    "    mask = mask_transform(mask)\n",
    "    mask = torch.squeeze(mask, 0)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "774ba002-b36c-4624-8acb-b9cd7fb183b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = get_target('/teamspace/studios/this_studio/Deeplab/crf_sam_annotations/binary_masks/eastafrica_acactort_2020.05.08.14.52.18_820f49c8-795b-453e-8eb1-7d473669ab84_img_20200507_152251_66198747_binarymask.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3e626af0-ce22-4f1c-85be-ac35cc6e0692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 512])\n"
     ]
    }
   ],
   "source": [
    "print(label.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f99b211-04dc-4c5d-bf24-690644cbb04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_to_grayscale_tensor(image):\n",
    "    grayscale_transform = transforms.Compose([\n",
    "        transforms.Grayscale(),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    return grayscale_transform(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be737657-97f9-411d-90f9-11ba4f78cd5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62214b20-0cef-45b7-bf57-8c8ea3f0e5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4])\n",
      "tensor([[ 46.,  17., 467., 483.]])\n"
     ]
    }
   ],
   "source": [
    "#gray_tensor_box = pil_to_grayscale_tensor(label)\n",
    "label_tensor = torch.tensor(label)\n",
    "label_tensor = label_tensor.unsqueeze(0) \n",
    "mask_tensor = mask_tensor.unsqueeze(0) \n",
    "boxes = masks_to_boxes(mask_tensor)\n",
    "print(boxes.size())\n",
    "print(boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84faebd0-7bb3-4359-8a39-92bd22a00895",
   "metadata": {},
   "outputs": [],
   "source": [
    "box = masks_to_boxes(mask_tensor.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2c865ad-0d4e-4115-ae60-db67df2e201d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 46.,  17., 467., 483.]])\n"
     ]
    }
   ],
   "source": [
    "print(box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c3cede6-1b36-4f20-be5c-328653990d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(boxes.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81d04d6f-678f-4278-be38-a9ac10bcafec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_to_grayscale_tensor(image):\n",
    "    grayscale_transform = transforms.Compose([\n",
    "        transforms.Grayscale(),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    return grayscale_transform(image)\n",
    "    \n",
    "def boxes_from_dataset(dataset_dir):\n",
    "    os.makedirs(os.path.join(dataset_dir,'boxes'),exist_ok=True)\n",
    "    # we assume you have a folder for binary masks called binary_masks\n",
    "    masks = os.listdir(os.path.join(dataset_dir,'binary_masks'))\n",
    "    for mask_path in masks:\n",
    "        mask = Image.open(os.path.join(dataset_dir,'binary_masks',mask_path))\n",
    "        box = masks_to_boxes(pil_to_grayscale_tensor(mask))\n",
    "        numpy_box = box.numpy()\n",
    "        txt_path = os.path.join(dataset_dir,'boxes',mask_path.split('_binarymask')[0]) + '.txt' \n",
    "        with open (txt_path, 'w') as f:\n",
    "            np.savetxt(txt_path, numpy_box)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1232a72-25e6-41ae-ba96-b61a5de5ce56",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes_from_dataset('/teamspace/studios/this_studio/Deeplab/crf_sam_annotations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53a7d1c-1912-491b-8f0c-d585958e52a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree('/teamspace/studios/this_studio/Deeplab/crf_sam_annotations/boxes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "fc7c4e67-f4e4-4196-8474-5f9ffe100479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0., 112., 295., 511.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the NumPy array from the text file binary_masks/eastafrica_acactort_2020.05.08.14.52.18_820f49c8-795b-453e-8eb1-7d473669ab84_img_20200507_152251_66198747_binarymask.jpg\n",
    "loaded_numpy_array = np.loadtxt('/teamspace/studios/this_studio/Deeplab/crf_sam_annotations/boxes/eastafrica_acactort_2020.05.08.14.52.18_820f49c8-795b-453e-8eb1-7d473669ab84_img_20200507_152251_66198747.txt')\n",
    "\n",
    "# Convert the NumPy array back to a tensor\n",
    "loaded_tensor = torch.from_numpy(loaded_numpy_array)\n",
    "print(loaded_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ea613109-3a52-439b-b0dc-05fc844f4591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "print(loaded_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6fbbf4ed-bf6d-40d6-bfaa-8d1304ba0131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "loaded_tensor = loaded_tensor.unsqueeze(0)\n",
    "print(loaded_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "477837d4-d0fd-4963-b573-f6bc647d9d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_dice_loss(pred, target, bounding_boxes, weight_factor=2.0):\n",
    "    \"\"\"\n",
    "    Calculate the weighted binary Dice loss with additional penalty for errors within bounding boxes.\n",
    "    \n",
    "    Parameters:\n",
    "    - pred: Predicted binary mask (tensor of shape [B, H, W])\n",
    "    - target: Ground truth binary mask (tensor of shape [B, H, W])\n",
    "    - bounding_boxes: List of bounding boxes in the format [x1, y1, x2, y2]\n",
    "    - weight_factor: Factor to increase the penalty within bounding boxes\n",
    "    \n",
    "    Returns:\n",
    "    - loss: Weighted binary Dice loss\n",
    "    \"\"\"\n",
    "    #BCE = nn.BCEWithLogitsLoss(pred,target)\n",
    "    #pred = torch.sigmoid(pred)\n",
    "    smooth = 1.0  # Smoothing factor to avoid division by zero\n",
    "\n",
    "    # Flatten the predictions and target masks\n",
    "    pred_flat = pred\n",
    "    target_flat = target\n",
    "    \n",
    "    # Create a mask for bounding boxes\n",
    "    bbox_mask = torch.zeros_like(pred, dtype=torch.float32)\n",
    "    \n",
    "    for bbox in bounding_boxes:\n",
    "        bbox = bbox.to(torch.int32)\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        print(bbox)\n",
    "        bbox_mask[:, y1:y2, x1:x2] = 0.0\n",
    "\n",
    "    bbox_mask_flat = bbox_mask\n",
    "    \n",
    "    # Weighted Dice coefficient calculation\n",
    "    intersection = torch.sum(pred_flat * target_flat * bbox_mask_flat) * weight_factor + torch.sum(pred_flat * target_flat * (1 - bbox_mask_flat))\n",
    "    dice = (2.0 * intersection + smooth) / (torch.sum(pred_flat * bbox_mask_flat) * weight_factor + torch.sum(pred_flat * (1 - bbox_mask_flat)) +\n",
    "                                            torch.sum(target_flat * bbox_mask_flat) * weight_factor + torch.sum(target_flat * (1 - bbox_mask_flat)) + smooth)\n",
    "    \n",
    "    loss = 1 - dice\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b825b992-df0c-4da6-9aed-002f5ccb8cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0, 112, 295, 511], dtype=torch.int32)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[132], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mbinary_dice_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloaded_tensor\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[131], line 29\u001b[0m, in \u001b[0;36mbinary_dice_loss\u001b[0;34m(pred, target, bounding_boxes, weight_factor)\u001b[0m\n\u001b[1;32m     27\u001b[0m     x1, y1, x2, y2 \u001b[38;5;241m=\u001b[39m bbox\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(bbox)\n\u001b[0;32m---> 29\u001b[0m     \u001b[43mbbox_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my1\u001b[49m\u001b[43m:\u001b[49m\u001b[43my2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m:\u001b[49m\u001b[43mx2\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     31\u001b[0m bbox_mask_flat \u001b[38;5;241m=\u001b[39m bbox_mask\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Weighted Dice coefficient calculation\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "loss = binary_dice_loss(output,label,loaded_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b287ea57-d3b9-44aa-9063-a89e8b871ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
