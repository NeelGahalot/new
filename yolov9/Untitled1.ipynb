{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa03c03c-ab72-4b88-a077-82c558f9199a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/teamspace/studios/this_studio/yolov9/weights/best.pt'], source=/teamspace/studios/this_studio/Deeplab/inhouse_test/samples/test_1.jpg, data=data/coco128.yaml, imgsz=[512, 512], conf_thres=0.1, iou_thres=0.45, max_det=1000, device=0, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
      "Fusing layers... \n",
      "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n",
      "image 1/1 /teamspace/studios/this_studio/Deeplab/inhouse_test/samples/test_1.jpg: 512x384 1 plant, 605.2ms\n",
      "Speed: 1.2ms pre-process, 605.2ms inference, 0.8ms NMS per image at shape (1, 3, 512, 512)\n",
      "Results saved to \u001b[1mruns/detect/exp\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python /teamspace/studios/this_studio/yolov9/detect.py \\\n",
    "--img 512 --conf 0.1 --device 0 \\\n",
    "--weights /teamspace/studios/this_studio/yolov9/weights/best.pt \\\n",
    "--source \"/teamspace/studios/this_studio/Deeplab/inhouse_test/samples/test_1.jpg\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca677bf3-e0ac-4671-85f3-9717857dc9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "ROOT = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef40b6c5-f472-4c79-ad0b-83e212fd63be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "sys.path.append(str(ROOT)) \n",
    "\n",
    "from models.common import DetectMultiBackend\n",
    "from utils.dataloaders import IMG_FORMATS, VID_FORMATS, LoadImages, LoadScreenshots, LoadStreams\n",
    "from utils.general import (LOGGER, Profile, check_file, check_img_size, check_imshow, check_requirements, colorstr, cv2,\n",
    "                           increment_path, non_max_suppression, print_args, scale_boxes, strip_optimizer, xyxy2xywh)\n",
    "from utils.plots import Annotator, colors, save_one_box\n",
    "from utils.torch_utils import select_device, smart_inference_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "318ff017-2e72-4f8f-bf09-75e459deef88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yolo.pt\n"
     ]
    }
   ],
   "source": [
    "ROOT = Path(os.path.relpath(ROOT, Path.cwd()))\n",
    "print(ROOT / 'yolo.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "899050e2-47cd-4fa0-aaba-03bb12f575d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "        weights= '/teamspace/studios/this_studio/yolov9/weights/best.pt'  # model path or triton URL\n",
    "        source= \"/teamspace/studios/this_studio/Deeplab/india_sam_dino_annotations/samples/india_psidguaj (guava)2020.10.30.15.45.36_25.237648598849773_79.32546760886908_53c529cf-f035-41e8-a90f-c3b159afef4c_img_20201030_153234_1625392956074806556.jpg\" # file/dir/URL/glob/screen/0(webcam)\n",
    "        data=ROOT / 'data/coco.yaml'  # dataset.yaml path\n",
    "        imgsz=(640, 640)  # inference size (height, width)\n",
    "        conf_thres=0.25  # confidence threshold\n",
    "        iou_thres=0.45  # NMS IOU threshold\n",
    "        max_det=1000  # maximum detections per image\n",
    "        device=''  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n",
    "        view_img=False  # show results\n",
    "        save_txt=False  # save results to *.txt\n",
    "        save_conf=False  # save confidences in --save-txt labels\n",
    "        save_crop=False  # save cropped prediction boxes\n",
    "        nosave=False  # do not save images/videos\n",
    "        classes=None  # filter by class: --class 0, or --class 0 2 3\n",
    "        agnostic_nms=False  # class-agnostic NMS\n",
    "        augment=False  # augmented inference\n",
    "        visualize=False  # visualize features\n",
    "        update=False  # update all models\n",
    "        project=ROOT / 'runs/detect'  # save results to project/name\n",
    "        name='exp'  # save results to project/name\n",
    "        exist_ok=False  # existing project/name ok, do not increment\n",
    "        line_thickness=3  # bounding box thickness (pixels)\n",
    "        hide_labels=False  # hide labels\n",
    "        hide_conf=False  # hide confidences\n",
    "        half=False  # use FP16 half-precision inference\n",
    "        dnn=False  # use OpenCV DNN for ONNX inference\n",
    "        vid_stride=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e58be81-d9c8-4119-bd38-df9369e3810c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(\n",
    "        weights=ROOT / 'yolo.pt',  # model path or triton URL\n",
    "        source=ROOT / 'data/images',  # file/dir/URL/glob/screen/0(webcam)\n",
    "        data=ROOT / 'data/coco.yaml',  # dataset.yaml path\n",
    "        imgsz=(640, 640),  # inference size (height, width)\n",
    "        conf_thres=0.25,  # confidence threshold\n",
    "        iou_thres=0.45,  # NMS IOU threshold\n",
    "        max_det=1000,  # maximum detections per image\n",
    "        device='',  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n",
    "        view_img=False,  # show results\n",
    "        save_txt=False,  # save results to *.txt\n",
    "        save_conf=False,  # save confidences in --save-txt labels\n",
    "        save_crop=False,  # save cropped prediction boxes\n",
    "        nosave=False,  # do not save images/videos\n",
    "        classes=None,  # filter by class: --class 0, or --class 0 2 3\n",
    "        agnostic_nms=False,  # class-agnostic NMS\n",
    "        augment=False,  # augmented inference\n",
    "        visualize=False,  # visualize features\n",
    "        update=False,  # update all models\n",
    "        project=ROOT / 'runs/detect',  # save results to project/name\n",
    "        name='exp',  # save results to project/name\n",
    "        exist_ok=False,  # existing project/name ok, do not increment\n",
    "        line_thickness=3,  # bounding box thickness (pixels)\n",
    "        hide_labels=False,  # hide labels\n",
    "        hide_conf=False,  # hide confidences\n",
    "        half=False,  # use FP16 half-precision inference\n",
    "        dnn=False,  # use OpenCV DNN for ONNX inference\n",
    "        vid_stride=1,  # video frame-rate stride\n",
    "):\n",
    "    source = str(source)\n",
    "    save_img = not nosave and not source.endswith('.txt')  # save inference images\n",
    "    is_file = Path(source).suffix[1:] in (IMG_FORMATS + VID_FORMATS)\n",
    "    is_url = source.lower().startswith(('rtsp://', 'rtmp://', 'http://', 'https://'))\n",
    "    #webcam = source.isnumeric() or source.endswith('.txt') or (is_url and not is_file)\n",
    "    screenshot = source.lower().startswith('screen')\n",
    "    if is_url and is_file:\n",
    "        source = check_file(source)  # download\n",
    "\n",
    "    # Directories\n",
    "    '''\n",
    "    save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)  # increment run\n",
    "    (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n",
    "    '''\n",
    "    # Load model\n",
    "    #device = select_device(device)\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)\n",
    "    stride, names, pt = model.stride, model.names, model.pt\n",
    "    imgsz = check_img_size(imgsz, s=stride)  # check image size\n",
    "\n",
    "    # Dataloader\n",
    "    dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)\n",
    "\n",
    "    # Run inference\n",
    "    model.warmup(imgsz=(1 if pt or model.triton else bs, 3, *imgsz))  # warmup\n",
    "    seen, windows, dt = 0, [], (Profile(), Profile(), Profile())\n",
    "    for path, im, im0s, vid_cap, s in dataset:\n",
    "        with dt[0]:\n",
    "            im = torch.from_numpy(im).to(model.device)\n",
    "            im = im.half() if model.fp16 else im.float()  # uint8 to fp16/32\n",
    "            im /= 255  # 0 - 255 to 0.0 - 1.0\n",
    "            if len(im.shape) == 3:\n",
    "                im = im[None]  # expand for batch dim\n",
    "\n",
    "        # Inference\n",
    "        with dt[1]:\n",
    "            visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False\n",
    "            pred = model(im, augment=augment, visualize=visualize)\n",
    "\n",
    "        # NMS\n",
    "        with dt[2]:\n",
    "            pred = pred[0][1] if isinstance(pred[0], list) else pred[0]\n",
    "            pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n",
    "\n",
    "        # Second-stage classifier (optional)\n",
    "        # pred = utils.general.apply_classifier(pred, classifier_model, im, im0s)\n",
    "\n",
    "        # Process predictions\n",
    "        for i, det in enumerate(pred):  # per image\n",
    "            seen += 1\n",
    "            p, im0, frame = path, im0s.copy(), getattr(dataset, 'frame', 0)\n",
    "\n",
    "            p = Path(p)  # to Path\n",
    "            #save_path = str(save_dir / p.name)  # im.jpg\n",
    "            #txt_path = str(save_dir / 'labels' / p.stem) + ('' if dataset.mode == 'image' else f'_{frame}')  # im.txt\n",
    "            s += '%gx%g ' % im.shape[2:]  # print string\n",
    "            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
    "            imc = im0.copy() if save_crop else im0  # for save_crop\n",
    "            annotator = Annotator(im0, line_width=line_thickness, example=str(names))\n",
    "            if len(det):\n",
    "                # Rescale boxes from img_size to im0 size\n",
    "                det[:, :4] = scale_boxes(im.shape[2:], det[:, :4], im0.shape).round()\n",
    "\n",
    "                # Write results\n",
    "                for *xyxy, conf, cls in reversed(det):\n",
    "                    xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()\n",
    "                    print(xywh)\n",
    "                    return xywh\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10f42a8b-df5c-4e0d-95e6-9465f2d8877a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_run(\n",
    "        weights='yolo.pt',  # model path or triton URL\n",
    "        source='data/images/image.jpg',  # file/URL\n",
    "        imgsz=(640, 640),  # inference size (height, width)\n",
    "        conf_thres=0.25,  # confidence threshold\n",
    "        iou_thres=0.45,  # NMS IOU threshold\n",
    "        max_det=1000,  # maximum detections per image\n",
    "        device='',  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n",
    "        save_crop=False,  # save cropped prediction boxes\n",
    "        line_thickness=3,  # bounding box thickness (pixels)\n",
    "        half=False,  # use FP16 half-precision inference\n",
    "        dnn=False,  # use OpenCV DNN for ONNX inference\n",
    "):\n",
    "    source = str(source)\n",
    "    is_file = Path(source).suffix[1:] in ('jpg', 'jpeg', 'png', 'bmp', 'tiff')\n",
    "    is_url = source.lower().startswith(('http://', 'https://'))\n",
    "    \n",
    "    if is_url and is_file:\n",
    "        source = check_file(source)  # download\n",
    "\n",
    "    # Load model\n",
    "    device = select_device(device)\n",
    "    model = DetectMultiBackend(weights, device=device, dnn=dnn)\n",
    "    stride, names, pt = model.stride, model.names, model.pt\n",
    "    imgsz = check_img_size(imgsz, s=stride)  # check image size\n",
    "\n",
    "    # Dataloader\n",
    "    dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt)\n",
    "    \n",
    "    # Run inference\n",
    "    model.warmup(imgsz=(1, 3, *imgsz))  # warmup\n",
    "    dt = Profile()\n",
    "    \n",
    "    for path, im, im0s, vid_cap, s in dataset:\n",
    "        with dt:\n",
    "            im = torch.from_numpy(im).to(model.device)\n",
    "            im = im.half() if model.fp16 else im.float()  # uint8 to fp16/32\n",
    "            im /= 255  # 0 - 255 to 0.0 - 1.0\n",
    "            if len(im.shape) == 3:\n",
    "                im = im[None]  # expand for batch dim\n",
    "\n",
    "        # Inference\n",
    "        pred = model(im, augment=False)\n",
    "\n",
    "        # NMS\n",
    "        pred = non_max_suppression(pred, conf_thres, iou_thres, max_det=max_det)\n",
    "\n",
    "        # Process predictions\n",
    "        for i, det in enumerate(pred):  # per image\n",
    "            p, im0 = path, im0s.copy()\n",
    "            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
    "            annotator = Annotator(im0, line_width=line_thickness, example=str(names))\n",
    "            \n",
    "            if len(det):\n",
    "                # Rescale boxes from img_size to im0 size\n",
    "                det[:, :4] = scale_boxes(im.shape[2:], det[:, :4], im0.shape).round()\n",
    "\n",
    "                # Write results\n",
    "                for *xyxy, conf, cls in reversed(det):\n",
    "                    xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()\n",
    "                    print(xywh)\n",
    "                return xywh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c13b6e65-508b-49f7-9de8-44024d291198",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m(\n\u001b[1;32m      2\u001b[0m     weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/teamspace/studios/this_studio/yolov9/weights/best.pt\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m     source\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/teamspace/studios/this_studio/Deeplab/india_sam_dino_annotations/samples/india_psidguaj (guava)2020.10.30.15.45.36_25.237648598849773_79.32546760886908_53c529cf-f035-41e8-a90f-c3b159afef4c_img_20201030_153234_1625392956074806556.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     data\u001b[38;5;241m=\u001b[39mROOT \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/coco.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m     imgsz\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m640\u001b[39m, \u001b[38;5;241m640\u001b[39m),\n\u001b[1;32m      6\u001b[0m     conf_thres\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.85\u001b[39m,\n\u001b[1;32m      7\u001b[0m     iou_thres\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.45\u001b[39m,\n\u001b[1;32m      8\u001b[0m     max_det\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      9\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     10\u001b[0m     view_img\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m     save_txt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m     save_conf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     13\u001b[0m     save_crop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m     nosave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     15\u001b[0m     classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     16\u001b[0m     agnostic_nms\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     17\u001b[0m     augment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     18\u001b[0m     visualize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     19\u001b[0m     update\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     20\u001b[0m     project\u001b[38;5;241m=\u001b[39mROOT \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mruns/detect\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     21\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexp\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     22\u001b[0m     exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     23\u001b[0m     line_thickness\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     24\u001b[0m     hide_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     25\u001b[0m     hide_conf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     26\u001b[0m     half\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     27\u001b[0m     dnn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     28\u001b[0m     vid_stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     29\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'run' is not defined"
     ]
    }
   ],
   "source": [
    "a = run(\n",
    "    weights='/teamspace/studios/this_studio/yolov9/weights/best.pt',\n",
    "    source=\"/teamspace/studios/this_studio/Deeplab/india_sam_dino_annotations/samples/india_psidguaj (guava)2020.10.30.15.45.36_25.237648598849773_79.32546760886908_53c529cf-f035-41e8-a90f-c3b159afef4c_img_20201030_153234_1625392956074806556.jpg\",\n",
    "    data=ROOT / 'data/coco.yaml',\n",
    "    imgsz=(640, 640),\n",
    "    conf_thres=0.85,\n",
    "    iou_thres=0.45,\n",
    "    max_det=1,\n",
    "    device='',\n",
    "    view_img=False,\n",
    "    save_txt=False,\n",
    "    save_conf=False,\n",
    "    save_crop=False,\n",
    "    nosave=False,\n",
    "    classes=None,\n",
    "    agnostic_nms=False,\n",
    "    augment=False,\n",
    "    visualize=False,\n",
    "    update=False,\n",
    "    project=ROOT / 'runs/detect',\n",
    "    name='exp',\n",
    "    exist_ok=False,\n",
    "    line_thickness=3,\n",
    "    hide_labels=False,\n",
    "    hide_conf=False,\n",
    "    half=False,\n",
    "    dnn=False,\n",
    "    vid_stride=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "485a9bc6-f29e-4926-98e0-165ba7720222",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 🚀 1e33dbb Python-3.10.10 torch-2.2.1+cu121 CPU\n",
      "\n",
      "Fusing layers... \n",
      "gelan-c summary: 467 layers, 25411731 parameters, 0 gradients, 102.5 GFLOPs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.41458332538604736, 0.09062500298023224, 0.8291666507720947, 0.18125000596046448]\n",
      "[0.4479166567325592, 0.2671875059604645, 0.8930555582046509, 0.534375011920929]\n"
     ]
    }
   ],
   "source": [
    "b = new_run(\n",
    "        weights='/teamspace/studios/this_studio/yolov9/weights/best.pt',  # model path or triton URL\n",
    "        source='/teamspace/studios/this_studio/test.jpg',  # file/URL\n",
    "        imgsz=(640, 640),  # inference size (height, width)\n",
    "        conf_thres=0.25,  # confidence threshold\n",
    "        iou_thres=0.45,  # NMS IOU threshold\n",
    "        max_det=1000,  # maximum detections per image\n",
    "        device='',  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n",
    "        save_crop=False,  # save cropped prediction boxes\n",
    "        line_thickness=3,  # bounding box thickness (pixels)\n",
    "        half=False,  # use FP16 half-precision inference\n",
    "        dnn=False,  # use OpenCV DNN for ONNX inference\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "19a222b8-8157-45e4-8264-a918a4f2c721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.326171875, 0.5419921875, 0.421875, 0.322265625]\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e1694f-22a8-4008-b582-b492ad4c2d98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
