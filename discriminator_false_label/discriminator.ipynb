{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3de39096-533f-4bea-8cab-feac60a4652d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "def split_dataset(image_dir, output_dir, train_ratio, val_ratio, test_ratio):\n",
    "    \n",
    "    images = [img for img in os.listdir(image_dir)]\n",
    "        \n",
    "    \n",
    "\n",
    "    # Adjust the extension\n",
    "    total_images = len(images)\n",
    "    random.shuffle(images)\n",
    "\n",
    "    train_size = int(total_images * train_ratio)\n",
    "    val_size = int(total_images * val_ratio)\n",
    "\n",
    "    # Split the dataset\n",
    "    train_images = images[:train_size]\n",
    "    val_images = images[train_size:train_size + val_size]\n",
    "    test_images = images[train_size + val_size:]\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    with open(os.path.join(output_dir, 'train.txt'), 'w') as f:\n",
    "      for item in train_images:\n",
    "        file_name_without_ext = item.lower().rsplit('.jpg', 1)[0]\n",
    "        f.write(\"%s\\n\" % file_name_without_ext)\n",
    "        #f.write(\"%s\\n\" % os.path.splitext(item)[0]) \n",
    "\n",
    "    with open(os.path.join(output_dir, 'val.txt'), 'w') as f:\n",
    "      for item in val_images:\n",
    "        file_name_without_ext = item.lower().rsplit('.jpg', 1)[0]\n",
    "        f.write(\"%s\\n\" % file_name_without_ext)\n",
    "        #f.write(\"%s\\n\" % os.path.splitext(item)[0]) \n",
    "        \n",
    "    with open(os.path.join(output_dir, 'test.txt'), 'w') as f:\n",
    "      for item in test_images:\n",
    "        file_name_without_ext = item.lower().rsplit('.jpg', 1)[0]\n",
    "        f.write(\"%s\\n\" % file_name_without_ext)\n",
    "        #f.write(\"%s\\n\" % os.path.splitext(item)[0])  \n",
    "\n",
    "    return train_images, val_images, test_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad57f878-31d2-4331-b191-2c3fbf3350cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['eastafrica_oleacape_2020.06.02.13.04.00_529ffc53-5bb9-4024-8e0e-fb0cdb1cbd21_img_20200529_160846_1078423866.jpg',\n",
       "  'haiti_acacauri_2021.05.31.10.44.04_18.28520064242184_-73.564688321203_f1b23d67-cfd2-4741-8975-715dc0f931f2_img_20210527_075726_1008487320000484652.jpg',\n",
       "  'eastafrica_afzeliaa_2023.03.30.15.58.48_2.2114935166666667_31.47824771666667_077a9822-e16b-4556-ab19-6be7d90aba72_img_20230328_132513_8240023535835035087.jpg',\n",
       "  'eastafrica_avicenni_2021.07.15.13.30.45_-5.125406691999997_39.11243287999997_d6587876-2bab-4ef3-b76d-55a4b682ed70_img_20210715_114220_323165884590609859.jpg',\n",
       "  'eastafrica_avicenni_2021.07.15.13.30.19_-5.126656833999998_39.11121906200001_748228c3-63bd-46e2-b0b2-ed9709c4e34c_img_20210715_113417_3011454690660837856.jpg',\n",
       "  'india_psidguaj (guava)2021.01.30.19.30.36_25.230063227936625_79.32936024852097_d68cf102-60d1-437e-ba5b-6570b1b3daf0_img_20210127_110234_1909062434403654501.jpg',\n",
       "  'eastafrica_cordafri_2022.08.25.13.24.26_-3.3070423333333316_37.29833933333336_5e7f9dc5-5d86-4e3b-bf1f-26a15ff4f8e1_img_20220823_085955_1005573694070971731.jpg',\n",
       "  'eastafrica_artohete_2022.12.22.10.02.57_0.6385307242665128_33.13795809301757_a2d27690-2fec-4fcf-8966-c19b72c406f5_img_20221220_112134_5327952274646282078.jpg',\n",
       "  'eastafrica_gmelarbo_2022.03.22.10.38.21_2.9513175_32.45036481666667_d392d906-8c63-46b9-8d25-e3bd05873f50_img_20220317_152534_8458642388097259419.jpg',\n",
       "  'eastafrica_gmelarbo_2022.03.24.06.55.28_2.9405984_32.344475949999996_71c97120-99fd-45ba-8cca-507b23f6aaba_img_20220314_142555_7984546332716014924.jpg',\n",
       "  'eastafrica_marklute_2020.05.13.14.22.04_27b3ee5f-45c8-4e1f-bbd3-b0d5c180a2e5_img_20200513_115047_7066454704550504882.jpg',\n",
       "  'eastafrica_marklute_2020.05.13.14.09.29_911b5770-7fb0-4c08-a04c-6e1665f4c4ae_img_20200513_112900_8277647013538329158.jpg',\n",
       "  'eastafrica_cedrodor_2020.07.30.20.27.34_1311b485-54ea-4ccc-98b5-d85eb4568f58_img_20200729_132409_736031792.jpg',\n",
       "  'eastafrica_marklute_2020.08.19.08.25.57_6ee53ee8-9367-49ec-919b-3228214e5f32_img_20200818_102957_6421708551130221069.jpg',\n",
       "  'freetown_tectgran_2020.11.10.15.57.59_8.456595_-13.232773333333334_c44b7234-b6b7-466c-8d4c-2c212f086851_img_20201110_120356_9198164571382559769.jpg',\n",
       "  'eastafrica_acactort_2020.08.27.17.13.48_d0c72c40-0fd8-4456-937c-a983343e43df_img_20200827_100055_357754425.jpg',\n",
       "  'eastafrica_afzeliaa_2023.03.30.15.57.44_2.205245183333333_31.50699123333333_31ec5cd9-8584-4e35-b08b-88bc69532732_img_20230328_102713_2517238834909540678.jpg',\n",
       "  'eastafrica_oleacape_2020.05.08.14.47.47_b2a68807-a438-4c4c-8950-94966e7ce4a8_img_20200507_135442_1081115120.jpg',\n",
       "  'eastafrica_cordafri_2022.06.28.16.25.09_-3.209681666666667_37.25638833333334_94eef062-5717-409f-acaa-66d5428c94a1_img_20220628_114546_7162188403002835653.jpg',\n",
       "  'eastafrica_artohete_2022.12.26.11.10.50_0.44920334201929074_33.17518843852042_8c9f2283-072b-43db-a0c9-5ae72c585876_img_20221226_104616_6868210532217254288.jpg',\n",
       "  'eastafrica_rauvcaff_2020.10.01.04.20.17_-3.1525116666666664_36.692038333333336_b912bbb7-d0f0-4214-86c8-8aec9111d4aa_img_20200929_164118_104931352.jpg',\n",
       "  'india_psidguaj (guava)2020.12.22.14.08.46_25.2426436_79.32886780799998_85c9a8d3-f9d1-45ae-98ea-d2b66d5fccce_img_20201221_105833_8831253614203653808.jpg',\n",
       "  'eastafrica_avicenni_2021.07.15.13.33.57_-5.121971512_39.11880127399999_7816d882-9936-4fc8-8fc3-89b80603bb11_img_20210715_121714_41515224628067237.jpg',\n",
       "  'eastafrica_acactort_2020.07.08.22.09.02_f64fac11-f5a3-4ed4-97cd-aafaf903a391_img_20100103_072427_1187961410.jpg',\n",
       "  'eastafrica_citr0000_2021.07.14.16.03.25_-3.2145516666666665_36.64796166666667_fe2f934e-3ded-4488-9d97-955c04971e78_img_20210714_132921_1460262207.jpg',\n",
       "  'eastafrica_persamer_2021.03.20.17.18.16_-9.351411666666673_34.54567000000001_57cfdc9e-ea9d-474d-8e00-623314afb983_img_20210320_123257_636184941472838791.jpg',\n",
       "  'india_psidguaj (guava)2020.12.22.19.22.59_25.242647314444184_79.32888784445822_30116e40-220e-4f50-8e17-55b196a28571_img_20201221_105456_5363774303356047617.jpg',\n",
       "  'eastafrica_citr0000_2021.07.10.09.24.35_-3.2143916666666663_36.637714999999965_2f6b81e4-c77e-47e7-9774-0bbeab251b6d_img_20210709_104637_1126380755.jpg',\n",
       "  'eastafrica_persamer_2020.06.10.14.22.21_2b780551-9729-42bc-83e6-b28740b26bb6_img_20200610_122856_-2107186059.jpg',\n",
       "  'eastafrica_marklute_2020.06.09.11.34.57_e627f520-dbad-4e82-bc80-2e9fb8279573_img_20200609_104340_3303551775199176660.jpg',\n",
       "  'eastafrica_albiziac_2023.01.03.22.32.35_-5.255151333333333_38.73491233333334_07e08975-7856-4fde-adb2-b7bec16f041d_img_20230103_090318_6992659408364799888.jpg',\n",
       "  'eastafrica_gmelarbo_2022.03.23.22.28.09_2.9515143333333325_32.450688_063febf4-44b0-4b88-bc0b-19da90e735e3_img_20220317_154319_7277074906806767578.jpg',\n",
       "  'eastafrica_afzeliaa_2023.03.30.15.58.48_2.2239973833333333_31.50621458333334_b1fe4394-61e6-4ac6-ae27-cd5858d4c3a4_img_20230330_135028_5062199739609010067.jpg',\n",
       "  'eastafrica_oleacape_2020.07.08.20.44.18_688db4b1-3512-4db6-adc0-668ed3cf41e2_img_20100101_092425_327828831.jpg',\n",
       "  'eastafrica_artohete_2022.12.22.10.02.57_0.6385121628088507_33.138007750828464_42daf886-1c57-43e9-a023-561a4dbff887_img_20221220_112819_541495579933556398.jpg',\n",
       "  'eastafrica_newtbuch_2022.08.24.12.07.04_-3.3068679999999997_37.298221_6f5117cf-3754-4abf-bb50-3b6d6f31df2f_img_20220823_084858_5637084825414702018.jpg',\n",
       "  'india_psidguaj (guava)2020.12.22.14.03.22_25.242524990000003_79.32898915999999_4ce657f9-2986-48f6-83bc-1945a54d199e_img_20201221_102853_4217613648042913635.jpg',\n",
       "  'eastafrica_citr0000_2020.09.07.11.25.35_d8cc025d-403f-4349-8eed-47edb74a13ab_img_20200907_112308_2329966240742820100.jpg',\n",
       "  'haiti_acacauri_2022.02.25.17.58.38_18.292734333333332_-73.555502_893ba163-9074-4b6a-bd6a-f160fcb0ce31_img_20220220_075623_198114764005333000.jpg',\n",
       "  'india_psidguaj (guava)2021.01.30.19.29.07_25.23004374001175_79.32934306561947_8349eb4c-3aaf-437b-a3d0-1202625873a7_img_20210127_105912_7726632931891423386.jpg',\n",
       "  'eastafrica_albiziac_2021.07.08.18.43.17_-4.809650666666668_38.262065666666686_330a5dbe-57b2-4494-a90a-f4cec647c4bf_img_20210708_145245_3186773890307053752.jpg',\n",
       "  'haiti_cedrodor_2021.10.12.12.22.26_18.291783085092902_-73.55496586672962_c3ed0b90-7fa9-431a-88bb-5eab6a2bdf58_img_20211007_094110_4012873410561694200.jpg',\n",
       "  'eastafrica_persamer_2021.03.20.19.32.13_-9.336433333333336_34.591435000000004_1432ddde-cd7b-4562-b394-9cce0619076e_img_20210320_153319_1194487046235466242.jpg',\n",
       "  'eastafrica_cordafri_2021.12.03.11.55.47_-3.2101373333333334_37.25609699999998_21ab116f-bc38-454c-ad4c-ea89d559f5b0_img_20211203_111657_4329943034561477090.jpg',\n",
       "  'eastafrica_cedrodor_2020.06.09.13.00.36_dc34b04b-51a3-4d9e-b8eb-0a0f14c9a51a_img_20200609_125231_7990858145161708233.jpg',\n",
       "  'eastafrica_albiziac_2023.01.04.06.30.41_-5.255195_38.73491833333333_26d89f1d-37d3-4049-aef7-d0c8c381fd98_img_20230103_090840_3739424836246171986.jpg',\n",
       "  'eastafrica_oleacape_2020.05.08.14.42.30_a3f54ac4-fb1a-43ea-ad9d-c96301416cd8_img_20200506_172609_914438899.jpg',\n",
       "  'eastafrica_cordafri_2022.03.01.13.06.08_-3.212344666666667_37.254674_4400fcdb-7a85-4cd6-b12e-7c99816e3e35_img_20220301_115301_4056100383859520584.jpg',\n",
       "  'eastafrica_afzeliaa_2023.04.03.18.25.49_1.717154222_31.402292998_df010cce-28c1-464d-b80a-78e0eec32f8f_img_20230403_162734_246077822516477126.jpg',\n",
       "  'eastafrica_cordafri_2022.08.23.19.29.34_-3.336656272_37.262059564_1e6a3e98-c03e-4fbd-967e-669e1e2b7b32_img_20220101_053053_6989571525748401880.jpg',\n",
       "  'eastafrica_artohete_2022.12.30.11.29.43_0.5884975492342732_33.43358038041392_36831cb4-707a-4a3c-865c-9311bae3876d_img_20221230_105425_5364068010691918528.jpg'],\n",
       " ['india_psidguaj (guava)2020.10.21.17.01.50_22.043116120621562_86.3750886451453_109e2e3c-ef60-4710-8058-5d5884f25038_img_20201020_140157_2815177404586427848.jpg',\n",
       "  'eastafrica_miliciae_2023.04.03.18.26.23_1.7202213499999999_31.385415958_334211a5-fb07-4ee4-9330-17596e61b13c_img_20230403_175006_2932375464717672784.jpg',\n",
       "  'eastafrica_afzeliaa_2023.03.30.15.57.44_2.2052450500000003_31.5069423_f949275f-5e85-4dc5-aac2-9c7cb17df704_img_20230328_102528_2712695584843967361.jpg',\n",
       "  'eastafrica_marklute_2020.05.12.13.55.41_6da492c3-eb4c-49d9-93ce-e7c845174d56_img_20200512_121346_2097596090.jpg',\n",
       "  'eastafrica_avicenni_2021.07.15.13.31.42_-5.125200378_39.11261986999999_1ff0dd5d-7284-4eff-bbf7-49b690bed388_img_20210715_115030_5892685703223618033.jpg',\n",
       "  'india_psidguaj (guava)2020.12.22.19.19.06_25.242520663887262_79.32903184555471_f0f15bf6-76d3-4267-950f-cbbead96b558_img_20201221_103227_7756145533517118511.jpg',\n",
       "  'eastafrica_gmelarbo_2022.04.01.22.08.35_3.0062269999999995_32.416356333333326_6f53eeac-888a-4ee3-a2ba-237af081e1d5_img_20220324_102755_2019117999404029592.jpg',\n",
       "  'eastafrica_acactort_2020.05.08.14.52.18_820f49c8-795b-453e-8eb1-7d473669ab84_img_20200507_152251_66198747.jpg',\n",
       "  'india_psidguaj (guava)2020.12.20.11.12.51_25.241541478_79.32896181600002_04528e7a-e029-4e02-bef4-850e88bb2e01_img_20201220_104940_5675156004412287130.jpg',\n",
       "  'eastafrica_gmelarbo_2022.04.01.22.58.38_2.89034_32.42089566666667_8b1a0e7d-250b-4227-8bde-1ce8715f12d4_img_20220401_133714_8552483403970629169.jpg',\n",
       "  'eastafrica_afzeliaa_2023.03.30.15.58.08_2.205202933333333_31.49989131666667_2a752ef7-f5fd-4a0e-8fee-45a395ea29b5_img_20230328_111116_3999528174717439203.jpg',\n",
       "  'eastafrica_afzeliaa_2023.03.30.15.58.48_2.21132695_31.47830271666667_64ac089f-3635-4e29-9c40-346ff6733640_img_20230328_133132_1029900243110606907.jpg',\n",
       "  'eastafrica_afzeliaa_2023.03.30.15.57.44_2.2058795_31.506689533333326_c22e65ec-7d94-4e40-a243-e549452d3352_img_20230328_103537_2217938063991025025.jpg'],\n",
       " ['india_psidguaj (guava)2021.01.20.10.48.14_22.09064333333334_88.76018333333332_2c691bbd-139c-46c7-bb52-2ae63e2dc41a_img_20210120_101934_9084359407528682683.jpg',\n",
       "  'india_psidguaj (guava)2020.12.27.08.39.51_25.235508615151048_79.32377806864679_a63df987-892d-4104-a188-9e9fb60bc6f4_img_20201226_103523_6413154955210011314.jpg',\n",
       "  'eastafrica_mangindi_2020.08.02.14.28.09_4274ab5b-099f-409e-9fdc-fe719ce3da14_img_20200802_095555_1957291619.jpg',\n",
       "  'india_psidguaj (guava)2021.01.20.10.48.10_22.090690000000002_88.76017666666667_b5da9b21-8c9b-4cdc-85b6-0a7b37587243_img_20210120_101840_4930256377871764840.jpg'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dataset('/teamspace/studios/this_studio/discriminator_false_label/dataset/samples', '/teamspace/studios/this_studio/discriminator_false_label/dataset/splits', 0.75, 0.2,0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "847c9bee-e1cc-40bd-a30d-911d65dbd57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "class LeafDataset(data.Dataset):\n",
    "    def __init__(self, root, image_set='train', img_transform=None, mask_transform=None):\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.img_transform = img_transform\n",
    "        self.mask_transform = mask_transform\n",
    "        self.image_set = image_set\n",
    "        \n",
    "        # Adjust paths\n",
    "        image_dir = os.path.join(self.root, 'samples')\n",
    "        mask_dir = os.path.join(self.root, 'binary_masks')\n",
    "        label_dir = os.path.join(self.root, 'labels')\n",
    "        split_fpath = os.path.join(self.root, 'splits', f'{self.image_set}.txt')        \n",
    "        \n",
    "        with open(split_fpath, 'r') as f:\n",
    "            file_names = [x.strip() for x in f.readlines()]        \n",
    "\n",
    "        self.images = [os.path.join(image_dir, fname + '.jpg') for fname in file_names]\n",
    "        self.masks = [os.path.join(mask_dir, fname + '_binarymask.jpg') for fname in file_names]\n",
    "        self.labels = [os.path.join(label_dir, fname + '_label.txt') for fname in file_names]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.images[index]\n",
    "        mask_path = self.masks[index]\n",
    "        label_path = self.labels[index]\n",
    "\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        mask = Image.open(mask_path)\n",
    "\n",
    "        mask_array = np.array(mask)\n",
    "        mask_array = (mask_array > 128).astype(np.uint8) # Binarize to 0s and 1s\n",
    "\n",
    "        mask_array = mask_array * 255\n",
    "        mask = Image.fromarray(mask_array.astype(np.uint8))\n",
    "        \n",
    "        if self.img_transform is not None:\n",
    "          img = self.img_transform(img)\n",
    "        \n",
    "        if self.mask_transform is not None:\n",
    "          mask = self.mask_transform(mask)\n",
    "\n",
    "        #mask = torch.squeeze(mask, 0)\n",
    "        if mask.shape[0] != 3:\n",
    "            mask = mask.repeat(3, 1, 1)\n",
    "\n",
    "        with open(label_path, 'r') as f:\n",
    "            label = f.readline()\n",
    "            label = int(label.strip()) \n",
    "            label = torch.tensor(1) if label == 1 else torch.tensor(0)\n",
    "\n",
    "        return img, mask, label\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fd07356-ff79-451e-b1e9-d4eadf560e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "train_img_transform = transforms.Compose([\n",
    "          #RandomCropAndPad(512),\n",
    "          transforms.Resize((512, 512)),\n",
    "          #transforms.RandomResizedCrop(size=(256, 256)),\n",
    "          #transforms.RandomHorizontalFlip(),\n",
    "          #transforms.RandomRotation(degrees=(0, 360)),\n",
    "          #transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n",
    "          transforms.ToTensor(),\n",
    "          transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "train_mask_transform = transforms.Compose([\n",
    "            #RandomCropAndPadMask(512),\n",
    "            transforms.Resize((512, 512), interpolation=transforms.InterpolationMode.NEAREST),\n",
    "            #transforms.RandomResizedCrop(size=(256, 256), interpolation=transforms.InterpolationMode.NEAREST),\n",
    "            #transforms.RandomHorizontalFlip(),\n",
    "            #transforms.RandomRotation(degrees=(0, 360)),\n",
    "            transforms.ToTensor(),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "110b0157-68c0-441c-a15a-1a42ff4488df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dst = LeafDataset(root= '/teamspace/studios/this_studio/discriminator_false_label/dataset/', image_set='train', img_transform=train_img_transform, mask_transform=train_mask_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7eb2096f-150f-443c-af30-91c976dbe103",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data.DataLoader(\n",
    "        train_dst, batch_size= 4, shuffle=True, num_workers=1,\n",
    "        drop_last=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef62a8e8-daec-4267-a55e-c6b69bf63ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89356cc7-81fd-47b1-b5e0-2ac02b99732f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 512, 512]) torch.Size([4, 3, 512, 512]) torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "for imgs, masks, labels in train_loader:\n",
    "    print(imgs.shape, masks.shape, labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1e050422-5ecd-40b6-bcd2-22fd2dc43473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "# Define your discriminator network\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(6, 16, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 64 * 64, 256)\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, img, mask):\n",
    "        if len(mask.shape) == 3:\n",
    "            mask = mask.unsqueeze(1)  # Add channel dimension if missing\n",
    "\n",
    "        x = torch.cat((img, mask), dim=1)  # Concatenate image and mask along channel dimension\n",
    "        x = F.leaky_relu(self.conv1(x), 0.2)\n",
    "        x = F.leaky_relu(self.conv2(x), 0.2)\n",
    "        x = F.leaky_relu(self.conv3(x), 0.2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
    "        x = self.fc2(x).squeeze(1)\n",
    "        return self.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "52643153-b114-421e-88e7-6760896ced1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Discriminator()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(discriminator.parameters(), lr=0.25, betas=(0.5, 0.999))\n",
    "\n",
    "def compute_loss(predictions, targets):\n",
    "    # predictions: tensor of shape (batch_size, ...)\n",
    "    # targets: tensor of shape (batch_size, ...), dtype Long\n",
    "\n",
    "    # Convert targets to float\n",
    "    targets = targets.float()\n",
    "\n",
    "    # Compute binary cross entropy loss\n",
    "    loss = F.binary_cross_entropy_with_logits(predictions, targets)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6969cac8-ff0b-4a57-a7b0-8c3723f5ecd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/discriminator_false_label\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5d66907e-44a7-47c6-a053-8eae953c8993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "interval loss 8.00613647699356\n",
      "saved.\n",
      "interval loss 7.119887351989746\n",
      "saved.\n",
      "interval loss 7.245870769023895\n",
      "saved.\n",
      "interval loss 7.372581660747528\n",
      "saved.\n",
      "interval loss 7.628607630729675\n",
      "interval loss 7.499457478523254\n",
      "saved.\n",
      "interval loss 7.371996104717255\n",
      "saved.\n",
      "interval loss 7.879944443702698\n",
      "saved.\n",
      "interval loss 7.7511221170425415\n",
      "saved.\n",
      "interval loss 7.499353766441345\n",
      "saved.\n",
      "interval loss 7.120481729507446\n",
      "interval loss 7.628564238548279\n",
      "saved.\n",
      "interval loss 7.626720368862152\n",
      "saved.\n",
      "interval loss 7.499725341796875\n",
      "saved.\n",
      "interval loss 7.625049352645874\n",
      "saved.\n",
      "interval loss 7.6290686428546906\n",
      "saved.\n",
      "interval loss 7.2455264031887054\n",
      "interval loss 7.373652338981628\n",
      "saved.\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "import torch\n",
    "import numpy as np\n",
    "num_epochs = 15\n",
    "cur_itrs = 0\n",
    "interval_loss = 0\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model = Discriminator().to(device)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for imgs,masks,labels in train_loader:\n",
    "        imgs, masks, labels = imgs.to(device), masks.to(device), labels.to(device)\n",
    "        cur_itrs += 1\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(imgs, masks)\n",
    "        #print(labels)\n",
    "        loss = compute_loss(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        np_loss = loss.detach().cpu().numpy()\n",
    "        #print(np_loss)\n",
    "        interval_loss += np_loss\n",
    "        del imgs, masks, labels, outputs, loss\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache() \n",
    "        if (cur_itrs) % 10 == 0:\n",
    "            print('interval loss ' + str(interval_loss))\n",
    "            interval_loss = 0.0\n",
    "    model_path = 'trial_1.pth'\n",
    "    optimizer_path = 'optimizer_trial_1.pth'\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    torch.save(optimizer.state_dict(), optimizer_path)\n",
    "    print('saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a3c1a9-a110-4ca4-bef3-8db4b53f1f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3b6fff55-ecae-4af8-91f9-b8efcfcb6bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dst = LeafDataset(root= '/teamspace/studios/this_studio/discriminator_false_label/dataset/', image_set='val', img_transform=train_img_transform, mask_transform=train_mask_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9a6aa793",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dst = LeafDataset(root= '/teamspace/studios/this_studio/discriminator_false_label/dataset/', image_set='test', img_transform=train_img_transform, mask_transform=train_mask_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "77f97c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "print(len(val_dst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "338ed160",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = data.DataLoader(\n",
    "        val_dst, batch_size= 4, shuffle=True, num_workers=1,\n",
    "        drop_last=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8d08238f-9cdd-469e-a54c-b8a16370c19c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (conv1): Conv2d(6, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=262144, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Discriminator().to(device)\n",
    "model.load_state_dict(torch.load('/teamspace/studios/this_studio/discriminator_false_label/trial_1.pth'))\n",
    "model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b5c85ae3-33ed-4d49-9f9e-c97dd33a23a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "    val_loss = 0\n",
    "    all_outputs = []\n",
    "    all_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "84ec4c30-abe0-424c-993e-6542532fa6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5107652  0.502795   0.5079358  0.5071701  0.5064978  0.5031692\n",
      " 0.50639796 0.50610954 0.50940406 0.50652844 0.50366265 0.5112094 ]\n",
      "[1 0 1 1 1 0 1 1 1 1 0 1]\n",
      "Validation Loss: 0.5973167816797892\n",
      "Validation Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "    with torch.no_grad():\n",
    "        for imgs, masks, labels in val_loader:\n",
    "            imgs, masks, labels = imgs.to(device), masks.to(device), labels.to(device)\n",
    "            outputs = model(imgs, masks)\n",
    "            loss = compute_loss(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            all_outputs.extend(outputs.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            # Clear memory\n",
    "            del imgs, masks, labels, outputs, loss\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    all_outputs = np.array(all_outputs)\n",
    "    print(all_outputs)\n",
    "    all_labels = np.array(all_labels)\n",
    "    print(all_labels)\n",
    "\n",
    "    # Compute additional metrics like accuracy, precision, recall, etc.\n",
    "    predictions = (all_outputs > 0.5).astype(int)\n",
    "    accuracy = np.mean(predictions == all_labels)\n",
    "    \n",
    "    print(f'Validation Loss: {avg_val_loss}')\n",
    "    print(f'Validation Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b215b26-fdc9-4d7f-ab3d-223904de4876",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3292c175-b991-47c1-b6d0-b9b4f096263a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 1, 0], device='cuda:0')\n",
      "tensor([1, 1, 1, 1], device='cuda:0')\n",
      "tensor([0, 1, 1, 1], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for imgs, masks, labels in val_loader:\n",
    "    labels = labels.to(device)\n",
    "    print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c1e21ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6988524198532104\n",
      "Validation Accuracy: 0.25\n"
     ]
    }
   ],
   "source": [
    "def validate_model(model, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    all_outputs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, masks, labels in val_loader:\n",
    "            imgs, masks, labels = imgs.to(device), masks.to(device), labels.to(device)\n",
    "            outputs = model(imgs, masks)\n",
    "            loss = compute_loss(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            all_outputs.extend(outputs.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            # Clear memory\n",
    "            del imgs, masks, labels, outputs, loss\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    all_outputs = np.array(all_outputs)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    # Compute additional metrics like accuracy, precision, recall, etc.\n",
    "    predictions = (all_outputs > 0.5).astype(int)\n",
    "    accuracy = np.mean(predictions == all_labels)\n",
    "    \n",
    "    print(f'Validation Loss: {avg_val_loss}')\n",
    "    print(f'Validation Accuracy: {accuracy}')\n",
    "\n",
    "    return avg_val_loss, accuracy\n",
    "\n",
    "# Assuming the rest of your setup is complete...\n",
    "# Validate the model\n",
    "val_loss, val_accuracy = validate_model(model, val_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baa6140",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
